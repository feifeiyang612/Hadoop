server.port=9090
#session超时时间设置，单位是：秒
server.session-timeout=300
#logging.level.org.springframework.web=DEBUG
#kafka，更多配置：org.springframework.boot.autoconfigure.kafka.KafkaProperties
#指定kafka 代理地址，可以多个
#spring.kafka.bootstrap-servers=172.21.32.82:9092,172.21.32.83:9093,172.21.32.84:9094
#指定默认topic id
#spring.kafka.template.default-topic=myTopic
#指定listener 容器中的线程数，用于提高并发量
#spring.kafka.listener.concurrency=3
#每次批量发送消息的数量
#spring.kafka.producer.batch-size=1000
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#指定默认消费者group id
#spring.kafka.consumer.group-id=myGroup1
#若设置为earliest，那么会从头开始读partition
#spring.kafka.consumer.auto-offset-reset=latest
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# 1、kafka0.8版本后参数改为bootstrap.servers，即元数据存储在Kafka集群上，旧的版本元数据存储在Zookeeper集群上；
# 2、该参数不用包含所有的brokers服务器，标注一个或者一个以上就可以；
kafka.producer.bootstrapServers=172.21.32.82:9092,172.21.32.83:9093,172.21.32.84:9094
#消息发送者发送消息失败后，消息重复发送次数
kafka.producer.retries=3
# producer会把发往同一分区的多条消息封装进一个batch中，当batch满了后，producer才会把消息发送出去。但是也不一定等到满了，这和另外一个参数linger.ms有关
# batch越小，producer的吞吐量越低，反之越大，吞吐量越大
#16K 默认值
kafka.producer.batchSize=16384
# 消息发送延时时间，单位是毫秒
kafka.producer.lingerMs=1
#该参数用于指定Producer端用于缓存消息的缓冲区大小，单位为字节，默认值为：33554432合计为32M。
#32M
kafka.producer.bufferMemory=33554432

kafka.consumer.bootstrapServers=172.21.32.82:9092,172.21.32.83:9093,172.21.32.84:9094
kafka.consumer.groupId=0
#kafka和springboot结合中的enable.auto.commit为false为spring的人工提交模式。enable.auto.commit为true是采用kafka的默认提交模式。
kafka.consumer.enableAutoCommit=false
#单位是毫秒
kafka.consumer.autoCommitIntervalMs=1000
#Consumer周期性的发送heartbeat到coordinator（协调者，在早期版本，以zookeeper作为协调者。后期版本则以某个broker作为协调者）。
#当Consumer由于某种原因不能发Heartbeat到coordinator时，并且时间超过session.timeout.ms时，就会认为该consumer已退出，它所订阅的partition会分配到同一group 内的其它的consumer上。
#单位：毫秒
kafka.consumer.sessionTimeoutMs=30000
#Consumer每次调用poll()时取到的records的最大数
kafka.consumer.maxPollRecords=100
#earliest,latest
#Kafka Broker在发现kafka在没有初始offset，或者当前的offset是一个不存在的值（如果一个record被删除，就肯定不存在了）时，该如何处理。
# 它的2种处理方式：1） earliest：自动重置到最早的offset。
#                  2） latest：看上去重置到最晚的offset。
kafka.consumer.autoOffsetReset=earliest